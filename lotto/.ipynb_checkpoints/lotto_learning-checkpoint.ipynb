{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\southman\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import  pandas as  pd\n",
    "from pandas import Series, DataFrame\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lotto_data = pd.read_csv('lotto_data.data', delimiter='\\t')\n",
    "lotto_data = lotto_data.sort_values('no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1th</th>\n",
       "      <th>2th</th>\n",
       "      <th>3th</th>\n",
       "      <th>4th</th>\n",
       "      <th>5th</th>\n",
       "      <th>6th</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>816</th>\n",
       "      <td>10</td>\n",
       "      <td>23</td>\n",
       "      <td>29</td>\n",
       "      <td>33</td>\n",
       "      <td>37</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>815</th>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>21</td>\n",
       "      <td>25</td>\n",
       "      <td>32</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>814</th>\n",
       "      <td>11</td>\n",
       "      <td>16</td>\n",
       "      <td>19</td>\n",
       "      <td>21</td>\n",
       "      <td>27</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>813</th>\n",
       "      <td>14</td>\n",
       "      <td>27</td>\n",
       "      <td>30</td>\n",
       "      <td>31</td>\n",
       "      <td>40</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>812</th>\n",
       "      <td>16</td>\n",
       "      <td>24</td>\n",
       "      <td>29</td>\n",
       "      <td>40</td>\n",
       "      <td>41</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>811</th>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "      <td>26</td>\n",
       "      <td>27</td>\n",
       "      <td>40</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>810</th>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>16</td>\n",
       "      <td>25</td>\n",
       "      <td>26</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>809</th>\n",
       "      <td>8</td>\n",
       "      <td>19</td>\n",
       "      <td>25</td>\n",
       "      <td>34</td>\n",
       "      <td>37</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>808</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>17</td>\n",
       "      <td>36</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>807</th>\n",
       "      <td>9</td>\n",
       "      <td>25</td>\n",
       "      <td>30</td>\n",
       "      <td>33</td>\n",
       "      <td>41</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>806</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>36</td>\n",
       "      <td>37</td>\n",
       "      <td>41</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>805</th>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>21</td>\n",
       "      <td>25</td>\n",
       "      <td>39</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>804</th>\n",
       "      <td>22</td>\n",
       "      <td>23</td>\n",
       "      <td>25</td>\n",
       "      <td>37</td>\n",
       "      <td>38</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>803</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "      <td>33</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>802</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>30</td>\n",
       "      <td>31</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>801</th>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>24</td>\n",
       "      <td>37</td>\n",
       "      <td>38</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>17</td>\n",
       "      <td>32</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799</th>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>19</td>\n",
       "      <td>32</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>6</td>\n",
       "      <td>30</td>\n",
       "      <td>38</td>\n",
       "      <td>39</td>\n",
       "      <td>40</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797</th>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>18</td>\n",
       "      <td>20</td>\n",
       "      <td>23</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>796</th>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>17</td>\n",
       "      <td>18</td>\n",
       "      <td>31</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>17</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>794</th>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>17</td>\n",
       "      <td>18</td>\n",
       "      <td>33</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>793</th>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>27</td>\n",
       "      <td>29</td>\n",
       "      <td>36</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>792</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>21</td>\n",
       "      <td>26</td>\n",
       "      <td>43</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>791</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>18</td>\n",
       "      <td>20</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>790</th>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>26</td>\n",
       "      <td>28</td>\n",
       "      <td>37</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>789</th>\n",
       "      <td>9</td>\n",
       "      <td>18</td>\n",
       "      <td>23</td>\n",
       "      <td>25</td>\n",
       "      <td>35</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>788</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>34</td>\n",
       "      <td>39</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>787</th>\n",
       "      <td>8</td>\n",
       "      <td>17</td>\n",
       "      <td>20</td>\n",
       "      <td>35</td>\n",
       "      <td>36</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>19</td>\n",
       "      <td>35</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>19</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>19</td>\n",
       "      <td>27</td>\n",
       "      <td>30</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "      <td>33</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>19</td>\n",
       "      <td>25</td>\n",
       "      <td>29</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>21</td>\n",
       "      <td>35</td>\n",
       "      <td>38</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>18</td>\n",
       "      <td>19</td>\n",
       "      <td>30</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>26</td>\n",
       "      <td>34</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>26</td>\n",
       "      <td>36</td>\n",
       "      <td>40</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>5</td>\n",
       "      <td>22</td>\n",
       "      <td>31</td>\n",
       "      <td>32</td>\n",
       "      <td>39</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>22</td>\n",
       "      <td>32</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>12</td>\n",
       "      <td>17</td>\n",
       "      <td>23</td>\n",
       "      <td>34</td>\n",
       "      <td>42</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>28</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>25</td>\n",
       "      <td>28</td>\n",
       "      <td>37</td>\n",
       "      <td>43</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>18</td>\n",
       "      <td>24</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>14</td>\n",
       "      <td>26</td>\n",
       "      <td>30</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>26</td>\n",
       "      <td>32</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>18</td>\n",
       "      <td>31</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>14</td>\n",
       "      <td>20</td>\n",
       "      <td>23</td>\n",
       "      <td>31</td>\n",
       "      <td>37</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>18</td>\n",
       "      <td>25</td>\n",
       "      <td>34</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>15</td>\n",
       "      <td>21</td>\n",
       "      <td>31</td>\n",
       "      <td>32</td>\n",
       "      <td>41</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>17</td>\n",
       "      <td>23</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>21</td>\n",
       "      <td>39</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>19</td>\n",
       "      <td>21</td>\n",
       "      <td>36</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "      <td>16</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>30</td>\n",
       "      <td>34</td>\n",
       "      <td>35</td>\n",
       "      <td>42</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>28</td>\n",
       "      <td>38</td>\n",
       "      <td>42</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17</td>\n",
       "      <td>21</td>\n",
       "      <td>25</td>\n",
       "      <td>26</td>\n",
       "      <td>27</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12</td>\n",
       "      <td>18</td>\n",
       "      <td>19</td>\n",
       "      <td>29</td>\n",
       "      <td>31</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>25</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>817 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     1th  2th  3th  4th  5th  6th\n",
       "816   10   23   29   33   37   40\n",
       "815    9   13   21   25   32   42\n",
       "814   11   16   19   21   27   31\n",
       "813   14   27   30   31   40   42\n",
       "812   16   24   29   40   41   42\n",
       "811   14   15   26   27   40   42\n",
       "810    2    9   16   25   26   40\n",
       "809    8   19   25   34   37   39\n",
       "808    2    4   16   17   36   39\n",
       "807    9   25   30   33   41   44\n",
       "806    1    7   36   37   41   42\n",
       "805    2   11   21   25   39   45\n",
       "804   22   23   25   37   38   42\n",
       "803    2    6   12   31   33   40\n",
       "802    3    4   16   30   31   37\n",
       "801    6    7   24   37   38   40\n",
       "800    3    4    9   17   32   37\n",
       "799    3   12   13   19   32   35\n",
       "798    6   30   38   39   40   43\n",
       "797   10   14   18   20   23   30\n",
       "796    6   12   17   18   31   32\n",
       "795    4    5    6    8   17   39\n",
       "794    5   13   17   18   33   42\n",
       "793    7    8   27   29   36   43\n",
       "792    2    4   21   26   43   44\n",
       "791    4    5    7   18   20   25\n",
       "790    1   20   26   28   37   43\n",
       "789    9   18   23   25   35   37\n",
       "788    1    5   13   34   39   40\n",
       "787    8   17   20   35   36   44\n",
       "..   ...  ...  ...  ...  ...  ...\n",
       "29     2   10   11   19   35   39\n",
       "28     2    6    7   12   19   45\n",
       "27     3    8   19   27   30   41\n",
       "26     2   10   12   31   33   42\n",
       "25     2    7   19   25   29   36\n",
       "24    10   15   21   35   38   43\n",
       "23     6    7   18   19   30   38\n",
       "22     3   10   13   26   34   38\n",
       "21     1   21   26   36   40   41\n",
       "20     5   22   31   32   39   45\n",
       "19     2   10   14   22   32   36\n",
       "18    12   17   23   34   42   45\n",
       "17     1    4   10   12   28   45\n",
       "16    17   25   28   37   43   44\n",
       "15    10   11   12   18   24   42\n",
       "14     5    9   14   26   30   43\n",
       "13     1   10   13   26   32   36\n",
       "12     3   12   13   18   31   32\n",
       "11    14   20   23   31   37   38\n",
       "10     6   10   18   25   34   35\n",
       "9     15   21   31   32   41   43\n",
       "8      6   11   15   17   23   40\n",
       "7      5   10   13   21   39   43\n",
       "6      8   11   19   21   36   45\n",
       "5      1    3   12   14   16   43\n",
       "4     11   30   34   35   42   44\n",
       "3      2   21   28   38   42   45\n",
       "2     17   21   25   26   27   36\n",
       "1     12   18   19   29   31   39\n",
       "0      3    9   12   13   25   43\n",
       "\n",
       "[817 rows x 6 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lotto_data[['1th', '2th', '3th', '4th', '5th', '6th']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\southman\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"strided_slice:0\", shape=(?, 5, 64), dtype=float32)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Only call `softmax_cross_entropy_with_logits` with named arguments (labels=..., logits=..., ...)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-56-85e4b2e2a945>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[1;31m#print(dataX,dataY)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[0mdata_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msplit_train_test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 191\u001b[1;33m \u001b[0mlet_leaning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-56-85e4b2e2a945>\u001b[0m in \u001b[0;36mlet_leaning\u001b[1;34m(data_params)\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[1;31m# 그래프를 그리고 학습을 시킨다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mlet_leaning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 185\u001b[1;33m     \u001b[0mgraph_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdraw_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mlet_training\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgraph_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-56-85e4b2e2a945>\u001b[0m in \u001b[0;36mdraw_graph\u001b[1;34m()\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdynamic_rnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstacked_rnn_cell\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m     \u001b[0mY_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfully_connected\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'output_dim'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax_cross_entropy_with_logits_v2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[1;31m# cost/loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\framework\\python\\ops\\arg_scope.py\u001b[0m in \u001b[0;36mfunc_with_args\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    181\u001b[0m       \u001b[0mcurrent_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcurrent_scope\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey_func\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m       \u001b[0mcurrent_args\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 183\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcurrent_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    184\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m   \u001b[0m_add_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\layers\\python\\layers\\layers.py\u001b[0m in \u001b[0;36mfully_connected\u001b[1;34m(inputs, num_outputs, activation_fn, normalizer_fn, normalizer_params, weights_initializer, weights_regularizer, biases_initializer, biases_regularizer, reuse, variables_collections, outputs_collections, trainable, scope)\u001b[0m\n\u001b[0;32m   1728\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1729\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mactivation_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1730\u001b[1;33m       \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mactivation_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1731\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1732\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect_named_outputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs_collections\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\u001b[0m in \u001b[0;36msoftmax_cross_entropy_with_logits_v2\u001b[1;34m(_sentinel, labels, logits, dim, name)\u001b[0m\n\u001b[0;32m   1827\u001b[0m   \"\"\"\n\u001b[0;32m   1828\u001b[0m   _ensure_xent_args(\"softmax_cross_entropy_with_logits\", _sentinel, labels,\n\u001b[1;32m-> 1829\u001b[1;33m                     logits)\n\u001b[0m\u001b[0;32m   1830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1831\u001b[0m   \u001b[1;31m# TODO(pcmurray) Raise an error when the labels do not sum to 1. Note: This\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\u001b[0m in \u001b[0;36m_ensure_xent_args\u001b[1;34m(name, sentinel, labels, logits)\u001b[0m\n\u001b[0;32m   1773\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0msentinel\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1774\u001b[0m     raise ValueError(\"Only call `%s` with \"\n\u001b[1;32m-> 1775\u001b[1;33m                      \"named arguments (labels=..., logits=..., ...)\" % name)\n\u001b[0m\u001b[0;32m   1776\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mlogits\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1777\u001b[0m     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Both labels and logits must be provided.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Only call `softmax_cross_entropy_with_logits` with named arguments (labels=..., logits=..., ...)"
     ]
    }
   ],
   "source": [
    "# matrix 데이터로 변경한다.\n",
    "def to_ndarray(cols_data) :\n",
    "    if isinstance(cols_data, Series):\n",
    "        return np.reshape(list(cols_data), (-1,1))\n",
    "    elif isinstance(cols_data, DataFrame):\n",
    "        return cols_data.as_matrix()\n",
    "\n",
    "def get_lotto_data() :\n",
    "    lotto_data = pd.read_csv('lotto_data.data', delimiter='\\t')\n",
    "    return lotto_data.sort_values('no')\n",
    "\n",
    "# RNN을 위한 데이터로 만든다. \n",
    "def get_dataXY(data) :\n",
    "    data = data[['1th', '2th', '3th', '4th', '5th', '6th']]/45\n",
    "    #print(data)\n",
    "    data = data.sort_values(by=1, axis=1)\n",
    "    x = to_ndarray(data)\n",
    "    \n",
    "    dataX = []\n",
    "    dataY = []\n",
    "    seq_length = params['seq_length']\n",
    "    x_len = len(x)\n",
    "    for i in range(0, x_len - seq_length-1):\n",
    "        _x = x[i:i + seq_length]\n",
    "        _y = x[i + seq_length] # Next close price\n",
    "        dataX.append(_x)\n",
    "        dataY.append(_y)\n",
    "    return dataX, dataY\n",
    "\n",
    "# train 및 test 데이터로 나눈다.\n",
    "def split_train_test(dataX, dataY) :\n",
    "    seq_length = params['seq_length']\n",
    "    data_count = len(dataY);\n",
    "    train_size = int(data_count * params['train_percent'] / 100)\n",
    "    \n",
    "    trainX = np.array(dataX[0:train_size])\n",
    "    testX = np.array(dataX[train_size:])\n",
    "    \n",
    "    trainY = np.array(dataY[0:train_size])\n",
    "    testY = np.array(dataY[train_size:])\n",
    "    \n",
    "    return {\n",
    "        'trainX': trainX, 'trainY': trainY, \n",
    "        'testX': testX, 'testY': testY \n",
    "    }\n",
    "\n",
    "# 텐스플로우 변수관계 그래프롤 그린다.\n",
    "def draw_graph() :\n",
    "    seq_length = params['seq_length']\n",
    "    data_dim = params['data_dim']\n",
    "    hidden_dims = params['hidden_dims']\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    X = tf.placeholder(tf.float32, [None, seq_length, data_dim])\n",
    "    Y = tf.placeholder(tf.float32, [None, params['output_dim']])\n",
    "    output_keep_prob = tf.placeholder(tf.float32)\n",
    "    \n",
    "    cells = []\n",
    "    for n in hidden_dims :\n",
    "        cell = tf.contrib.rnn.BasicLSTMCell(num_units=n, activation=tf.tanh)\n",
    "        dropout_cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=output_keep_prob)\n",
    "        cells.append(dropout_cell)\n",
    "    stacked_rnn_cell = tf.nn.rnn_cell.MultiRNNCell(cells)\n",
    "    outputs, _states = tf.nn.dynamic_rnn(stacked_rnn_cell, X, dtype=tf.float32)\n",
    "    outputs = tf.transpose(outputs, [1, 0, 2])\n",
    "    last = tf.gather(outputs, int(outputs.get_shape()[0]) - 1)\n",
    "    \n",
    "    Y_pred = tf.contrib.layers.fully_connected (outputs, params['output_dim'], activation_fn=None) \n",
    "\n",
    "    # cost/loss\n",
    "    loss = tf.reduce_sum(tf.square(Y_pred - Y))\n",
    "    \n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(params['learning_rate'])\n",
    "    train = optimizer.minimize(loss)\n",
    "\n",
    "    # RMSE\n",
    "    targets = tf.placeholder(tf.float32, [None, params['output_dim']])\n",
    "    predictions = tf.placeholder(tf.float32, [None, params['output_dim']])\n",
    "    rmse = tf.sqrt(tf.reduce_mean(predictions - targets))\n",
    "    \n",
    "    return {\n",
    "        'X': X,\n",
    "        'Y': Y,\n",
    "        'output_keep_prob': output_keep_prob,\n",
    "        'train': train,\n",
    "        'loss' : loss,\n",
    "        'Y_pred': Y_pred,\n",
    "        'targets': targets,\n",
    "        'rmse' : rmse,\n",
    "        'predictions': predictions,\n",
    "    }\n",
    "\n",
    "def draw_plot(rmse_vals, test_predict, invest_predicts, data_params) :\n",
    "    testY = data_params['testY']\n",
    "    investY = data_params['investY']\n",
    "    y = np.append(testY,investY)\n",
    "    predict =  np.append(test_predict, invest_predicts)\n",
    "    \n",
    "    mpl.rcParams['axes.unicode_minus'] = False\n",
    "    font_name = fm.FontProperties(fname=params['kor_font_path'], size=50).get_name()\n",
    "    plt.rc('font', family=font_name)\n",
    "    \n",
    "    plt.figure(1)\n",
    "    plt.plot(rmse_vals, 'gold')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Root Mean Square Error')\n",
    "    \n",
    "    plt.figure(2)\n",
    "    plt.plot(y, 'b')\n",
    "    plt.plot(predict, 'r')\n",
    "    plt.xlabel('Time Period')\n",
    "    plt.ylabel('Stock Price')\n",
    "    plt.show()\n",
    "\n",
    "def save_learning_image(sess, saver, graph_params) :\n",
    "    X = graph_params['X']\n",
    "    Y = graph_params['Y']\n",
    "    train = graph_params['train']\n",
    "    Y_pred = graph_params['Y_pred']\n",
    "    output_keep_prob = graph_params['output_keep_prob']\n",
    "    \n",
    "    tf.add_to_collection(\"X\", X)\n",
    "    tf.add_to_collection(\"Y\", Y)\n",
    "    tf.add_to_collection(\"train\", train)\n",
    "    tf.add_to_collection(\"Y_pred\", Y_pred)\n",
    "    tf.add_to_collection(\"output_keep_prob\", output_keep_prob)\n",
    "    saver.save(sess, \"./sessions/lotto.ckpt\")\n",
    "    \n",
    "# 학습을 시킨다.\n",
    "def let_training(graph_params, data_params) :\n",
    "    X = graph_params['X']\n",
    "    Y = graph_params['Y']\n",
    "    output_keep_prob = graph_params['output_keep_prob']\n",
    "    train = graph_params['train']\n",
    "    loss = graph_params['loss']\n",
    "    trainX = data_params['trainX']\n",
    "    trainY = data_params['trainY']\n",
    "    testX = data_params['testX']\n",
    "    testY = data_params['testY']\n",
    "    \n",
    "    Y_pred = graph_params['Y_pred']\n",
    "    targets = graph_params['targets']\n",
    "    rmse = graph_params['rmse']\n",
    "    predictions = graph_params['predictions']\n",
    "    loss_up_count = params['loss_up_count']\n",
    "    dropout_keep = params['dropout_keep']\n",
    "    iterations = params['iterations']\n",
    "    rmse_max = params['rmse_max']\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess.run(init)\n",
    "\n",
    "        # Training step\n",
    "        max_test_predict = []\n",
    "        min_rmse_val = 999999\n",
    "        less_cnt = 0\n",
    "        train_count = 0\n",
    "        rmse_vals = []\n",
    "        \n",
    "        for i in range(iterations[1]):\n",
    "            _, step_loss = sess.run([train, loss], feed_dict={X: trainX, Y: trainY, output_keep_prob: dropout_keep})\n",
    "            test_predict = sess.run(Y_pred, feed_dict={X: testX, output_keep_prob: 1.0})\n",
    "            #print(test_predict*45);\n",
    "            rmse_val = sess.run(rmse, feed_dict={targets: testY, predictions: test_predict}) \n",
    "            #print(rmse_val);\n",
    "            rmse_vals.append(rmse_val)\n",
    "            if rmse_val < min_rmse_val :\n",
    "                save_learning_image(sess, saver, graph_params)\n",
    "                less_cnt = 0\n",
    "                train_count = i;\n",
    "                #print(test_predict)\n",
    "                max_test_predict, min_rmse_val = test_predict, rmse_val\n",
    "            else :\n",
    "                less_cnt += 1\n",
    "            if i > iterations[0] and less_cnt > loss_up_count and rmse_max > min_rmse_val:\n",
    "                break\n",
    "        #draw_plot(rmse_vals, max_test_predict, testY, comp_name) \n",
    "        return min_rmse_val, train_count, rmse_vals, max_test_predict\n",
    "\n",
    "\n",
    "# 그래프를 그리고 학습을 시킨다.    \n",
    "def let_leaning(data_params):\n",
    "    graph_params = draw_graph()\n",
    "    return let_training(graph_params, data_params)\n",
    "\n",
    "dataX, dataY = get_dataXY(lotto_data)\n",
    "#print(dataX,dataY)\n",
    "data_params = split_train_test(dataX, dataY)\n",
    "let_leaning(data_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'seq_length' : 5, # 시퀀스 갯수\n",
    "    'data_dim' : 6,    # 입력 데이터 갯수\n",
    "    'hidden_dims' : [128, 96, 64],  # 히든 레이어 갯수 \n",
    "    'dropout_keep' : 0.8, # dropout \n",
    "    'output_dim' : 6,  # 출력 데이터 갯수\n",
    "    'learning_rate' : 0.0001, \n",
    "    'iterations' : [10, 200],  # 최소, 최대 훈련 반복횟수\n",
    "    'rmse_max' : 0.045,\n",
    "    'train_percent' : 80.0, # 훈련 데이터 퍼센트\n",
    "    'loss_up_count' : 12, # early stopping\n",
    "    'kor_font_path' : 'C:\\\\WINDOWS\\\\Fonts\\\\H2GTRM.TTF'\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
