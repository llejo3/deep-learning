{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting finance-datareader\n",
      "  Downloading https://files.pythonhosted.org/packages/47/41/00f1387673a5f42034228f541a2650a1c0b87fab8d65363c67df677e2a98/finance_datareader-0.5.0-py3-none-any.whl\n",
      "Collecting requests-file (from finance-datareader)\n",
      "  Downloading https://files.pythonhosted.org/packages/23/9c/6e63c23c39e53d3df41c77a3d05a49a42c4e1383a6d2a5e3233161b89dbf/requests_file-1.4.3-py2.py3-none-any.whl\n",
      "Requirement already satisfied: requests>=2.3.0 in c:\\users\\south\\anaconda3\\lib\\site-packages (from finance-datareader) (2.18.4)\n",
      "Requirement already satisfied: lxml in c:\\users\\south\\anaconda3\\lib\\site-packages (from finance-datareader) (4.1.1)\n",
      "Requirement already satisfied: pandas>=0.19.2 in c:\\users\\south\\anaconda3\\lib\\site-packages (from finance-datareader) (0.20.1)\n",
      "Requirement already satisfied: six in c:\\users\\south\\anaconda3\\lib\\site-packages (from requests-file->finance-datareader) (1.11.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\south\\anaconda3\\lib\\site-packages (from requests>=2.3.0->finance-datareader) (3.0.4)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in c:\\users\\south\\anaconda3\\lib\\site-packages (from requests>=2.3.0->finance-datareader) (2.6)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in c:\\users\\south\\anaconda3\\lib\\site-packages (from requests>=2.3.0->finance-datareader) (1.22)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\south\\anaconda3\\lib\\site-packages (from requests>=2.3.0->finance-datareader) (2018.4.16)\n",
      "Requirement already satisfied: python-dateutil>=2 in c:\\users\\south\\anaconda3\\lib\\site-packages (from pandas>=0.19.2->finance-datareader) (2.6.1)\n",
      "Requirement already satisfied: pytz>=2011k in c:\\users\\south\\anaconda3\\lib\\site-packages (from pandas>=0.19.2->finance-datareader) (2017.3)\n",
      "Requirement already satisfied: numpy>=1.7.0 in c:\\users\\south\\anaconda3\\lib\\site-packages (from pandas>=0.19.2->finance-datareader) (1.14.3)\n",
      "Installing collected packages: requests-file, finance-datareader\n",
      "Successfully installed finance-datareader-0.5.0 requests-file-1.4.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "grpcio 1.11.0 has requirement protobuf>=3.5.0.post1, but you'll have protobuf 3.4.1 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install finance-datareader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "import os\n",
    "import datetime\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "import matplotlib as mpl\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문자열을 데이터 형대로 변환한다.\n",
    "def to_date(date_str) :\n",
    "    date_str = date_str.replace(\" \", \"\")\n",
    "    split = \"\"\n",
    "    if date_str.find(\"-\") > -1 :\n",
    "        split = \"-\"\n",
    "    elif date_str.find(\".\") > -1 :\n",
    "        split = \".\"\n",
    "    date_format = '%Y'+ split + '%m' + split + '%d'    \n",
    "    return datetime.datetime.strptime(date_str, date_format)\n",
    "\n",
    "# excel로 저장한다.\n",
    "def save_excel(df_data, file_path):\n",
    "    writer = pd.ExcelWriter(file_path)\n",
    "    df_data.to_excel(writer,'Sheet1', index=False)\n",
    "    writer.save()\n",
    "\n",
    "# 주식회사 정보를 가져와서 엑셀로 저장한다.    \n",
    "def save_comp_data() :\n",
    "    url = 'http://kind.krx.co.kr/corpgeneral/corpList.do?method=download&searchType=13'\n",
    "    code_df = pd.read_html(url, header=0)[0]\n",
    "    save_excel(code_df, './data/comps.xlsx')\n",
    "\n",
    "# 엘셀을 불러와서 회사 코드를 가져온다.    \n",
    "def get_comp_code(comp_name) :\n",
    "    file_path = './data/comps.xlsx'\n",
    "    \n",
    "    if os.path.isfile(file_path) == False :\n",
    "        save_comp_data()\n",
    "        \n",
    "    corporations = pd.read_excel(file_path)\n",
    "    comp_code = corporations.query(\"회사명=='{}'\".format(comp_name))['종목코드'].to_string(index=False)\n",
    "    return format(int(comp_code), \"06d\")\n",
    "\n",
    "# 네이버 금융(http://finance.naver.com)에 넣어줌\n",
    "def get_naver_url(comp_code):\n",
    "    return 'http://finance.naver.com/item/sise_day.nhn?code={code}'.format(code=comp_code)\n",
    "\n",
    "# 네이버 매일 주식정보를 가져온다.\n",
    "def get_stock_naver_data(comp_code, start_date) :\n",
    "    url = get_naver_url(comp_code)\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    # 네이버 웹 크롤링\n",
    "    page = 1\n",
    "    bf_date = ''\n",
    "    while True :\n",
    "        pg_url = '{url}&page={page}'.format(url=url, page=page)\n",
    "        page_data = pd.read_html(pg_url, header=0)[0]\n",
    "        page_data = page_data.dropna()\n",
    "        last_date = page_data.tail(1)['날짜'].to_string(index=False)\n",
    "        if bf_date == last_date :\n",
    "            break\n",
    "        df = df.append(page_data, ignore_index=True)\n",
    "        if start_date != '' :\n",
    "            if to_date(start_date) > to_date(last_date) :\n",
    "                break\n",
    "        if len(page_data) < 10 :\n",
    "            break        \n",
    "        page += 1\n",
    "        bf_date = last_date \n",
    "    \n",
    "    # 필요 없는 날짜 제거\n",
    "    if start_date != '' :\n",
    "        drop_cnt = 0\n",
    "        df_len = len(df)\n",
    "        for i in range(df_len) :\n",
    "            last_date = df.loc[df_len-i-1, '날짜']\n",
    "            if to_date(start_date) > to_date(last_date) :\n",
    "                drop_cnt += 1\n",
    "            else :\n",
    "                break\n",
    "        if drop_cnt > 0 :        \n",
    "            df = df[:-drop_cnt]\n",
    "    \n",
    "    # 정렬 및 컬럼명 변경 \n",
    "    if df.shape[0] != 0 :\n",
    "        df = df.sort_values(by='날짜')\n",
    "        df.rename(columns={'날짜': 'date', \n",
    "                           '종가': 'close', \n",
    "                           '전일비': 'diff', \n",
    "                           '시가': 'open', \n",
    "                           '고가': 'high',\n",
    "                           '저가': 'low', \n",
    "                           '거래량': 'volume'}, inplace=True)    \n",
    "    return df\n",
    "    \n",
    "def get_stock_data(comp_code) :\n",
    "    file_path = './data/' + comp_code + '.csv'\n",
    "    \n",
    "    if os.path.isfile(file_path) :\n",
    "        stock_data = pd.read_csv(file_path)\n",
    "        date_last = stock_data.tail(1)['date'].to_string(index=False)\n",
    "        date_next = to_date(date_last) + datetime.timedelta(days=1)\n",
    "        date_next = date_next.strftime(\"%Y-%m-%d\")\n",
    "        date_now = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "        if date_next != date_now :\n",
    "            new_data = get_stock_naver_data(comp_code, date_next)\n",
    "            if len(new_data) > 0 :\n",
    "                stock_data = stock_data.append(new_data)\n",
    "                stock_data.to_csv(file_path, index=False)\n",
    "    else :\n",
    "        stock_data = get_stock_naver_data(comp_code, '')\n",
    "        stock_data.to_csv(file_path, index=False)\n",
    "    return stock_data\n",
    "\n",
    "# matrix 데이터로 변경한다.\n",
    "def to_ndarray(cols_data) :\n",
    "    if isinstance(cols_data, Series):\n",
    "        return np.reshape(list(cols_data), (-1,1))\n",
    "    elif isinstance(cols_data, DataFrame):\n",
    "        return cols_data.as_matrix()\n",
    "\n",
    "# 컬럼을 스케일링을 시킨다.\n",
    "def get_scaled_cols(data, column_name) :\n",
    "    scale_data = to_ndarray(data[column_name])\n",
    "    scaler = preprocessing.MinMaxScaler()\n",
    "    return scaler.fit_transform(scale_data), scaler;\n",
    "\n",
    "# 데이터를 스케일링 시킨다.\n",
    "def get_scaled_data(data) :\n",
    "    scaled_data = data.copy()\n",
    "    scaled_data = scaled_data[['close', 'open', 'high', 'low', 'volume']]\n",
    "    scaled_data = scaled_data[scaled_data['close']!=0]\n",
    "    scaled_data['close'], scaler_close = get_scaled_cols(scaled_data, 'close')\n",
    "    scaled_data['open'], _ = get_scaled_cols(scaled_data, 'open')\n",
    "    scaled_data['high'], _ = get_scaled_cols(scaled_data, 'high')\n",
    "    scaled_data['low'], _ = get_scaled_cols(scaled_data, 'low')\n",
    "    scaled_data['volume'], _ = get_scaled_cols(scaled_data, 'volume')\n",
    "    return scaled_data, scaler_close;\n",
    "\n",
    "# RNN을 위한 데이터로 만든다. \n",
    "def get_dataXY(data) :\n",
    "    x = to_ndarray(data)\n",
    "    y = to_ndarray(data['close'])\n",
    "    \n",
    "    dataX = []\n",
    "    dataY = []\n",
    "    seq_length = params['seq_length']\n",
    "    y_len = len(y)\n",
    "    for i in range(0, y_len - seq_length):\n",
    "        _x = x[i:i + seq_length]\n",
    "        _y = y[i + seq_length] # Next close price\n",
    "        dataX.append(_x)\n",
    "        dataY.append(_y)\n",
    "    dataX_last = [x[y_len-seq_length: y_len]]    \n",
    "    return dataX, dataY, y, dataX_last\n",
    "\n",
    "# train 및 test 데이터로 나눈다.\n",
    "def split_train_test(dataX, dataY, data, y) :\n",
    "    invest_count = params['invest_count']\n",
    "    seq_length = params['seq_length']\n",
    "    data_count = len(dataY);\n",
    "    train_size = int(data_count * params['train_percent'] / 100)\n",
    "    train_last = data_count-invest_count;\n",
    "    \n",
    "    trainX = np.array(dataX[0:train_size])\n",
    "    testX = np.array(dataX[train_size:train_last])\n",
    "    investX = np.array(dataX[train_last:data_count])\n",
    "    \n",
    "    trainY = np.array(dataY[0:train_size])\n",
    "    testY = np.array(dataY[train_size:train_last])\n",
    "    investY = np.array(dataY[train_last:data_count])\n",
    "    \n",
    "    trainCloses = np.array( y[seq_length-1:train_size+seq_length-1])\n",
    "    testCloses = np.array(dataY[train_size-1:train_last-1])\n",
    "    investCloses = np.array(dataY[train_last-1:data_count-1])\n",
    "    investRealCloses = np.array(data['close'][train_last-1+seq_length:data_count-1+seq_length].values)\n",
    "    \n",
    "    return {\n",
    "        'trainX': trainX, 'trainY': trainY, 'trainCloses': trainCloses,\n",
    "        'testX': testX, 'testY': testY, 'testCloses' : testCloses,\n",
    "        'investX': investX,'investY': investY, 'investCloses': investCloses, 'investRealCloses': investRealCloses\n",
    "    }\n",
    "\n",
    "# train, test데이터로 만든다.\n",
    "def get_train_test(data) :\n",
    "    scaled_data, scaler_close = get_scaled_data(data)\n",
    "    dataX, dataY, y, dataX_last = get_dataXY(scaled_data)\n",
    "    return split_train_test(dataX, dataY, data, y), scaler_close, dataX_last\n",
    "\n",
    "# 텐스플로우 변수관계 그래프롤 그린다.\n",
    "def draw_graph() :\n",
    "    seq_length = params['seq_length']\n",
    "    data_dim = params['data_dim']\n",
    "    hidden_dims = params['hidden_dims']\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    X = tf.placeholder(tf.float32, [None, seq_length, data_dim])\n",
    "    X_closes = tf.placeholder(tf.float32, [None, 1])\n",
    "    Y = tf.placeholder(tf.float32, [None, 1])\n",
    "    output_keep_prob = tf.placeholder(tf.float32)\n",
    "    \n",
    "    cells = []\n",
    "    for n in hidden_dims :\n",
    "        cell = tf.contrib.rnn.BasicLSTMCell(num_units=n, activation=tf.tanh)\n",
    "        dropout_cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=output_keep_prob)\n",
    "        cells.append(dropout_cell)\n",
    "    stacked_rnn_cell = tf.nn.rnn_cell.MultiRNNCell(cells)\n",
    "    outputs, _states = tf.nn.dynamic_rnn(stacked_rnn_cell, X, dtype=tf.float32) \n",
    "    Y_pred = tf.contrib.layers.fully_connected(\n",
    "        outputs[:, -1], params['output_dim'], activation_fn=None)  # We use the last cell's output\n",
    "\n",
    "    # cost/loss\n",
    "    loss = tf.reduce_sum(tf.square(1-(1+Y_pred-X_closes)/(1+Y-X_closes)))\n",
    "        \n",
    "    optimizer = tf.train.AdamOptimizer(params['learning_rate'])\n",
    "    train = optimizer.minimize(loss)\n",
    "\n",
    "    # RMSE\n",
    "    targets = tf.placeholder(tf.float32, [None, 1])\n",
    "    predictions = tf.placeholder(tf.float32, [None, 1])\n",
    "    rmse = tf.sqrt(tf.reduce_mean(tf.square(1-(1+predictions-X_closes)/(1+targets-X_closes))))\n",
    "    \n",
    "    return {\n",
    "        'X': X,\n",
    "        'Y': Y,\n",
    "        'output_keep_prob': output_keep_prob,\n",
    "        'train': train,\n",
    "        'loss' : loss,\n",
    "        'Y_pred': Y_pred,\n",
    "        'targets': targets,\n",
    "        'rmse' : rmse,\n",
    "        'predictions': predictions,\n",
    "        'X_closes' : X_closes\n",
    "    }\n",
    "\n",
    "def draw_plot(rmse_vals, test_predict, testY, comp_name) :\n",
    "    \n",
    "    mpl.rcParams['axes.unicode_minus'] = False\n",
    "    font_name = fm.FontProperties(fname=params['kor_font_path'], size=50).get_name()\n",
    "    plt.rc('font', family=font_name)\n",
    "    \n",
    "    plt.figure(1)\n",
    "    plt.plot(rmse_vals, 'gold')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Root Mean Square Error')\n",
    "    plt.title(comp_name)\n",
    "\n",
    "    plt.figure(2)\n",
    "    plt.plot(testY, 'r')\n",
    "    plt.plot(test_predict, 'b')\n",
    "    plt.xlabel('Time Period')\n",
    "    plt.ylabel('Stock Price')\n",
    "    plt.title(comp_name)\n",
    "    plt.show()\n",
    "\n",
    "def save_learning_image(sess, saver, graph_params, comp_code) :\n",
    "    X = graph_params['X']\n",
    "    Y = graph_params['Y']\n",
    "    X_closes = graph_params['X_closes']\n",
    "    train = graph_params['train']\n",
    "    Y_pred = graph_params['Y_pred']\n",
    "    output_keep_prob = graph_params['output_keep_prob']\n",
    "    \n",
    "    tf.add_to_collection(\"X\", X)\n",
    "    tf.add_to_collection(\"X_closes\", X_closes)\n",
    "    tf.add_to_collection(\"Y\", Y)\n",
    "    tf.add_to_collection(\"train\", train)\n",
    "    tf.add_to_collection(\"Y_pred\", Y_pred)\n",
    "    tf.add_to_collection(\"output_keep_prob\", output_keep_prob)\n",
    "    saver.save(sess, \"./sessions/\" + comp_code + \".ckpt\")\n",
    "    \n",
    "# 학습을 시킨다.\n",
    "def let_training(graph_params, comp_code, comp_name) :\n",
    "    X = graph_params['X']\n",
    "    Y = graph_params['Y']\n",
    "    output_keep_prob = graph_params['output_keep_prob']\n",
    "    train = graph_params['train']\n",
    "    loss = graph_params['loss']\n",
    "    trainX = data_params['trainX']\n",
    "    trainY = data_params['trainY']\n",
    "    testX = data_params['testX']\n",
    "    testY = data_params['testY']\n",
    "    trainCloses = data_params['trainCloses']\n",
    "    testCloses = data_params['testCloses']\n",
    "    \n",
    "    Y_pred = graph_params['Y_pred']\n",
    "    targets = graph_params['targets']\n",
    "    rmse = graph_params['rmse']\n",
    "    predictions = graph_params['predictions']\n",
    "    X_closes = graph_params['X_closes']\n",
    "    loss_up_count = params['loss_up_count']\n",
    "    dropout_keep = params['dropout_keep']\n",
    "    iterations = params['iterations']\n",
    "    rmse_max = params['rmse_max']\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess.run(init)\n",
    "\n",
    "        # Training step\n",
    "        min_rmse_val = 999999\n",
    "        less_cnt = 0\n",
    "        train_count = 0\n",
    "        rmse_vals = []\n",
    "        \n",
    "        for i in range(iterations[1]):\n",
    "            _, step_loss = sess.run([train, loss], feed_dict={X: trainX, Y: trainY, X_closes: trainCloses, output_keep_prob: dropout_keep})\n",
    "            if (i < iterations[0]) :\n",
    "                continue\n",
    "            \n",
    "            test_predict = sess.run(Y_pred, feed_dict={X: testX, output_keep_prob: 1.0})\n",
    "            rmse_val = sess.run(rmse, feed_dict={targets: testY, predictions: test_predict, X_closes: testCloses}) \n",
    "            rmse_vals.append(rmse_val)\n",
    "            if rmse_val < min_rmse_val :\n",
    "                save_learning_image(sess, saver, graph_params, comp_code)\n",
    "                less_cnt = 0\n",
    "                train_count = i;\n",
    "                max_test_predict, min_rmse_val, = test_predict, rmse_val\n",
    "            else :\n",
    "                less_cnt += 1\n",
    "            if less_cnt > loss_up_count and rmse_max > min_rmse_val:\n",
    "                #print(less_cnt, rmse_val, train_count, i)\n",
    "                break\n",
    "        draw_plot(rmse_vals, max_test_predict, testY, comp_name) \n",
    "        return min_rmse_val, train_count \n",
    "\n",
    "\n",
    "# 그래프를 그리고 학습을 시킨다.    \n",
    "def let_leaning(comp_code, comp_name):\n",
    "    graph_params = draw_graph()\n",
    "    return let_training(graph_params, comp_code, comp_name)\n",
    "\n",
    "\n",
    "# 예측 값에 따라 매수 매도를 실행한다.    \n",
    "def let_invest_money(invest_predict, now_scaled_close, now_close, now_money, now_stock_cnt) :\n",
    "    seq_length = params['seq_length']\n",
    "    data_dim = params['data_dim']\n",
    "    fee_percent = params['fee_percent']\n",
    "    invest_min_percent = params['invest_min_percent']\n",
    "    \n",
    "    ratio = (invest_predict - now_scaled_close) /now_scaled_close * 100\n",
    "    \n",
    "    if ratio > invest_min_percent :\n",
    "        cnt = math.floor(now_money/now_close)\n",
    "        if cnt > 0 :\n",
    "            fee = now_close * fee_percent/100\n",
    "            now_money -= (now_close + fee) * cnt\n",
    "            now_stock_cnt += cnt\n",
    "    elif ratio < -invest_min_percent :\n",
    "        if now_stock_cnt > 0 :\n",
    "            now_money += to_money(now_close, now_stock_cnt, train_params)\n",
    "            now_stock_cnt = 0\n",
    "    #print(now_money, now_stock_cnt, now_scaled_close, invest_predict, data_params['testY'])\n",
    "    return now_money, now_stock_cnt\n",
    "\n",
    "# 주식매도를 해서 돈으로 바꾼다.\n",
    "def to_money(now_stock_cnt, now_close) :\n",
    "    money = 0\n",
    "    if now_stock_cnt > 0 :\n",
    "        fee_percent = params['fee_percent'] \n",
    "        tax_percent = params['tax_percent']\n",
    "        \n",
    "        fee = now_close * fee_percent/100\n",
    "        tax = now_close * tax_percent/100\n",
    "        money = (now_close - (fee + tax)) * now_stock_cnt\n",
    "    return money\n",
    "    \n",
    "# 학습 후 모의 주식 거래를 한다.\n",
    "def let_invest(comp_code, train_cnt, dataX_last):\n",
    "    invest_count = params['invest_count']\n",
    "    invest_money = params['invest_money']\n",
    "    dropout_keep = params['dropout_keep']\n",
    "    \n",
    "    investX = data_params['investX']\n",
    "    investCloses = data_params['investCloses']\n",
    "    investRealCloses = data_params['investRealCloses']\n",
    "    investX = data_params['investX']\n",
    "    investY = data_params['investY']\n",
    "    testX = data_params['testX']\n",
    "    testY = data_params['testY']\n",
    "    testCloses = data_params['testCloses']\n",
    "    #print(investRealCloses)\n",
    "    \n",
    "    now_stock_cnt = 0\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess.run(init)\n",
    "        \n",
    "        saver.restore(sess, \"./sessions/\" + comp_code + \".ckpt\") \n",
    "        X = tf.get_collection('X')[0]\n",
    "        X_closes = tf.get_collection('X_closes')[0]\n",
    "        Y = tf.get_collection('Y')[0]\n",
    "        train = tf.get_collection('train')[0]\n",
    "        Y_pred = tf.get_collection('Y_pred')[0]\n",
    "        output_keep_prob = tf.get_collection('output_keep_prob')[0]\n",
    "        \n",
    "        for i in range(int(train_cnt/2)):\n",
    "            sess.run(train, feed_dict={X: testX, Y: testY, X_closes: testCloses, \n",
    "                                       output_keep_prob: dropout_keep})\n",
    "        \n",
    "        for i in range(invest_count) :\n",
    "            np.array([1, 2, 3], ndmin=2)\n",
    "            invest_predicts = sess.run(Y_pred, feed_dict={X: investX[i:i+1], output_keep_prob: 1.0})\n",
    "            \n",
    "            invest_predict = invest_predicts[0][0];\n",
    "            now_scaled_close = investCloses[0][0]\n",
    "            now_close = investRealCloses[i]\n",
    "            #print(invest_predict, now_scaled_close, now_close)\n",
    "            invest_money, now_stock_cnt = let_invest_money(invest_predict, now_scaled_close, now_close,\n",
    "                                                           invest_money, now_stock_cnt)\n",
    "            for i in range(int(train_cnt/5)):\n",
    "                sess.run(train, feed_dict={X: investX[i:i+1], Y: investY[i:i+1], X_closes: investCloses[i:i+1], \n",
    "                                           output_keep_prob: dropout_keep})\n",
    "            #break\n",
    "        invest_money += to_money(now_stock_cnt, now_close)\n",
    "        graph_params = {'X':X, 'X_closes':X_closes, 'Y':Y, 'train':train, \n",
    "                        'Y_pred':Y_pred, 'output_keep_prob':output_keep_prob}\n",
    "        save_learning_image(sess, saver, graph_params, comp_code)\n",
    "        saver.save(sess, \"./sessions/\" + comp_code + \".ckpt\")\n",
    "        \n",
    "        last_predict = sess.run(Y_pred, feed_dict={X: dataX_last, output_keep_prob: 1.0})\n",
    "    #print(now_money)\n",
    "    return invest_money, last_predict\n",
    "\n",
    "def get_real_money(data_params, scaler_close) :\n",
    "    investRealCloses = data_params['investRealCloses'];\n",
    "    predict_money = scaler_close.inverse_transform(last_predict)\n",
    "    last_close_money = investRealCloses[len(investRealCloses)-1]\n",
    "    last_pred_money = predict_money[0][0]\n",
    "    return last_close_money, last_pred_money"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'seq_length' : 5, # 시퀀스 갯수\n",
    "    'data_dim' : 5,    # 입력 데이터 갯수\n",
    "    'hidden_dims' : [128, 96, 64],  # 히든 레이어 갯수 \n",
    "    'dropout_keep' : 0.8, # dropout \n",
    "    'output_dim' : 1,  # 출력 데이터 갯수\n",
    "    'learning_rate' : 0.0001, \n",
    "    'iterations' : [30, 120],  # 최소, 최대 훈련 반복횟수\n",
    "    'rmse_max' : 0.049,\n",
    "    'train_percent' : 70.0, # 훈련 데이터 퍼센트\n",
    "    'loss_up_count' : 12, # early stopping\n",
    "    'invest_count' : 20,  # 투자 횟수\n",
    "    'invest_money' : 1000000, # 각 주식에 모의투자할 금액\n",
    "    'fee_percent' : 0.015, # 투자시 발생하는 수수료\n",
    "    'tax_percent' : 0.5,   # 매도시 발생하는 세금\n",
    "    'invest_min_percent' : 0.6, # 투자를 하는 최소 간격 퍼센트 \n",
    "    'kor_font_path' : 'C:\\\\WINDOWS\\\\Fonts\\\\H2GTRM.TTF'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-237-78368caa69a1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mcomp_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"현대엘리베이터\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mcomp_code\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_comp_code\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcomp_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mstock_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_stock_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcomp_code\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mdata_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscaler_close\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataX_last\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_train_test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstock_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mrmse_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_cnt\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mlet_leaning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcomp_code\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcomp_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-228-69336137df80>\u001b[0m in \u001b[0;36mget_stock_data\u001b[1;34m(comp_code)\u001b[0m\n\u001b[0;32m     97\u001b[0m                 \u001b[0mstock_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[1;32melse\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m         \u001b[0mstock_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_stock_naver_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcomp_code\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m         \u001b[0mstock_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mstock_data\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-228-69336137df80>\u001b[0m in \u001b[0;36mget_stock_naver_data\u001b[1;34m(comp_code, start_date)\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[0mpg_url\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'{url}&page={page}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m         \u001b[0mpage_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_html\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpg_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m         \u001b[0mpage_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpage_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpage_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\html.py\u001b[0m in \u001b[0;36mread_html\u001b[1;34m(io, match, flavor, header, index_col, skiprows, attrs, parse_dates, tupleize_cols, thousands, encoding, decimal, converters, na_values, keep_default_na)\u001b[0m\n\u001b[0;32m    904\u001b[0m                   \u001b[0mthousands\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mthousands\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    905\u001b[0m                   \u001b[0mdecimal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdecimal\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconverters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconverters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mna_values\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mna_values\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 906\u001b[1;33m                   keep_default_na=keep_default_na)\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\html.py\u001b[0m in \u001b[0;36m_parse\u001b[1;34m(flavor, io, match, attrs, encoding, **kwargs)\u001b[0m\n\u001b[0;32m    735\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    736\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 737\u001b[1;33m             \u001b[0mtables\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse_tables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    738\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcaught\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    739\u001b[0m             \u001b[0mretained\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcaught\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\html.py\u001b[0m in \u001b[0;36mparse_tables\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mparse_tables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 197\u001b[1;33m         \u001b[0mtables\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parse_tables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_build_doc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    198\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_build_table\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtable\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtable\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\html.py\u001b[0m in \u001b[0;36m_build_doc\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    555\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    556\u001b[0m             \u001b[1;31m# try to parse the input in the simplest way\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 557\u001b[1;33m             \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparser\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    558\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    559\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\lxml\\html\\__init__.py\u001b[0m in \u001b[0;36mparse\u001b[1;34m(filename_or_url, parser, base_url, **kw)\u001b[0m\n\u001b[0;32m    938\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mparser\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    939\u001b[0m         \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhtml_parser\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 940\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0metree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename_or_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbase_url\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbase_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    941\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    942\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32msrc/lxml/etree.pyx\u001b[0m in \u001b[0;36mlxml.etree.parse (src\\lxml\\etree.c:83198)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32msrc/lxml/etree.pyx\u001b[0m in \u001b[0;36mlxml.etree._elementTreeFactory (src\\lxml\\etree.c:70655)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32msrc/lxml/etree.pyx\u001b[0m in \u001b[0;36mlxml.etree._newElementTree (src\\lxml\\etree.c:70759)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32msrc/lxml/etree.pyx\u001b[0m in \u001b[0;36mlxml.etree._Document.getroot (src\\lxml\\etree.c:47699)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32msrc/lxml/etree.pyx\u001b[0m in \u001b[0;36mlxml.etree._elementFactory (src\\lxml\\etree.c:61570)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32msrc/lxml/classlookup.pxi\u001b[0m in \u001b[0;36mlxml.etree._parser_class_lookup (src\\lxml\\etree.c:94655)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32msrc/lxml/classlookup.pxi\u001b[0m in \u001b[0;36mlxml.etree._custom_class_lookup (src\\lxml\\etree.c:95143)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\lxml\\html\\__init__.py\u001b[0m in \u001b[0;36mlookup\u001b[1;34m(self, node_type, document, namespace, name)\u001b[0m\n\u001b[0;32m    734\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_element_classes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclasses\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    735\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 736\u001b[1;33m     \u001b[1;32mdef\u001b[0m \u001b[0mlookup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnode_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdocument\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnamespace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    737\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnode_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'element'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    738\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_element_classes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mHtmlElement\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "comp_name = \"현대엘리베이터\"\n",
    "comp_code = get_comp_code(comp_name)\n",
    "stock_data = get_stock_data(comp_code)\n",
    "data_params, scaler_close, dataX_last = get_train_test(stock_data)\n",
    "rmse_val, train_cnt  = let_leaning(comp_code, comp_name)\n",
    "last_money, last_predict = let_invest(comp_code, train_cnt, dataX_last)\n",
    "last_close_money, last_pred_money = get_real_money(data_params, scaler_close)\n",
    "print(\"RMSE:\", rmse_val)\n",
    "print(\"train_cnt:\", train_cnt)\n",
    "print(\"한달 모의투자 결과(100만원 투자):\", last_money)\n",
    "print(\"마지막 종가:\", \"{:,.0f}\".format(last_close_money))\n",
    "last_pred_ratio = (last_pred_money-last_close_money)/last_close_money * 100\n",
    "last_pred_ratio = \"(\" + \"{:.2f}\".format(last_pred_ratio) + \"%)\"\n",
    "print(\"예측 종가:\", \"{:,.0f}\".format( last_pred_money ), last_pred_ratio)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
